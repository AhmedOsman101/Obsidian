# Memory Management Concepts and Architectures

### Executive Summary

This document provides a comprehensive overview of main memory management, a critical function of modern operating systems. The core challenge is managing the finite main memory, the only storage besides registers that the CPU can access directly. A central concept is the distinction between a **logical address** (or virtual address), generated by the CPU, and a **physical address**, seen by the memory unit. The mapping between these two, known as **address binding**, can occur at compile time, load time, or, most flexibly, at execution time, which requires hardware support from a Memory-Management Unit (MMU).

Key memory management schemes have evolved to address efficiency and protection. Early systems used **Contiguous Allocation**, where each process occupies a single, unbroken block of memory. This simple method suffers from **external fragmentation**, where free memory is broken into small, non-contiguous pieces. This led to the development of **Paging** and **Segmentation**. Paging divides memory into fixed-size blocks (frames and pages), eliminating external fragmentation but introducing potential **internal fragmentation**. Segmentation aligns memory allocation with the user's view of a program (e.g., code, data, stack), but still faces external fragmentation. To accelerate address translation in paged systems, a hardware cache called the **Translation Look-aside Buffer (TLB)** is used. For large, 64-bit address spaces, standard page tables become impractically large, necessitating advanced structures like **Hierarchical Paging**, **Hashed Page Tables**, and **Inverted Page Tables**. Finally, **Swapping** allows processes to be moved to a backing store to free up main memory, though its high overhead makes it unsuitable for mobile systems, which use alternative strategies.

## Thematic Breakdown

### 1. Fundamental Concepts of Memory Management

#### The Memory Hierarchy and Program Execution

For a program to run, it must be loaded from a disk into main memory. The CPU can only directly access main memory and its own registers.

- **Registers:** Access is extremely fast, typically within one CPU clock cycle or less.
- **Main Memory:** Access is significantly slower, taking many CPU cycles and potentially causing the CPU to stall.
- **Cache:** Sits between the CPU registers and main memory to bridge the speed gap, holding frequently accessed data.

A crucial function of the operating system is to protect the memory space of each process from others to ensure correct operation.

#### Address Binding

Address binding is the process of mapping program addresses from one address space to another. This occurs at different stages of a program's lifecycle:

1. **Compile Time:** If the memory location is known in advance, the compiler can generate _absolute code_ with fixed physical addresses. If the starting location changes, the code must be recompiled.
2. **Load Time:** If the memory location is not known at compile time, the compiler must generate _relocatable code_. The final binding to physical addresses is delayed until the program is loaded into memory.
3. **Execution Time:** If a process can be moved during its execution from one memory segment to another, binding is delayed until runtime. This is the most flexible approach and requires hardware support, such as base and limit registers.

The typical multi-step process for a user program involves compiling source code into an object module, linking it with other modules and libraries to create a load module, and finally loading it into memory to create a binary memory image for execution.

#### Logical vs. Physical Address Space

This distinction is central to memory management, particularly with execution-time binding.

- **Logical Address (Virtual Address):** An address generated by the CPU. The set of all logical addresses generated by a program forms its _logical address space_. The user program deals only with logical addresses.
- **Physical Address:** An address seen by the memory unit (i.e., the actual address in the memory hardware). The set of all corresponding physical addresses forms the _physical address space_.

In compile-time and load-time binding, logical and physical addresses are the same. In execution-time binding, they differ.

#### The Memory-Management Unit (MMU)

The MMU is a hardware device responsible for translating logical (virtual) addresses to physical addresses at runtime. A simple MMU scheme uses a **relocation register** (a type of base register). The value in the relocation register is added to every logical address generated by a user process to produce the final physical address.

### 2. Memory Allocation and Protection Techniques

#### Hardware Address Protection

To protect processes from each other and the operating system, hardware support is essential. This is commonly implemented using two registers:

- **Base Register:** Stores the smallest legal physical memory address for a process.
- **Limit Register:** Stores the size of the process's logical address range.

For every memory access in user mode, the CPU hardware checks if the address is between the base and the (base + limit). Any attempt to access memory outside this range generates a trap to the operating system as an addressing error.

#### Contiguous Memory Allocation

An early memory management method where main memory is divided into two partitions: one for the resident operating system and one for user processes. Each user process is loaded into a single, contiguous section of memory.

- **Multiple-partition Allocation:** To support multiprogramming, the user space is divided into multiple partitions. This leads to the creation of **holes**—blocks of available memory scattered throughout. When a process arrives, it is allocated memory from a hole large enough to accommodate it.
- **Dynamic Storage-Allocation Problem:** This problem addresses how to satisfy a request for memory of size _n_ from a list of free holes. Common algorithms include:
  - **First-fit:** Allocate the first hole that is big enough.
  - **Best-fit:** Allocate the smallest hole that is big enough. Requires searching the entire list unless it is ordered by size.
  - **Worst-fit:** Allocate the largest hole. Also requires a full search.
- **Performance:** First-fit and best-fit are generally superior to worst-fit in terms of speed and storage utilization.

#### Fragmentation

Contiguous allocation suffers from fragmentation, which wastes memory.

- **External Fragmentation:** Exists when there is enough total free memory to satisfy a request, but it is not contiguous. It is scattered in small, separate holes.
- **Internal Fragmentation:** Occurs when allocated memory is slightly larger than the requested memory. The size difference is memory that is _internal_ to a partition but is not being used.
- **Compaction:** A solution to external fragmentation that involves shuffling memory contents to place all free memory together in one large block. This is only possible if relocation is dynamic (i.e., execution-time binding).

#### Swapping

Swapping is a mechanism where a process can be temporarily moved from main memory to a **backing store** (a fast disk) and brought back later for continued execution. This allows the total physical memory space of all processes to exceed the actual physical memory available.

- **Process:** A priority-based scheduler might use a "roll out, roll in" variant to swap out a lower-priority process to make room for a higher-priority one.
- **Overhead:** The major part of swap time is data transfer time, which is proportional to the amount of memory being swapped. This can make context switches very slow (e.g., 4 seconds for swapping a 100MB process).
- **Constraints:** A process with pending I/O cannot be swapped out easily, as the I/O would occur to the wrong memory space. This can be managed with double buffering (transferring I/O to kernel space first), which adds overhead.
- **Modern Usage:** Standard swapping is not common in modern OSes. Modified versions are used, typically only activating swapping when free memory is extremely low.
- **Swapping on Mobile Systems:** Swapping is generally not supported on mobile devices due to the limitations of flash memory (small space, limited write cycles, poor throughput). Instead, mobile OSes like iOS and Android use other strategies:
  - **iOS:** Asks apps to voluntarily relinquish memory. If they fail to do so, they may be terminated.
  - **Android:** Terminates apps if memory is low but first saves their state to flash for a fast restart.

### 3. Advanced Memory Management Schemes

#### Paging

Paging is a memory-management scheme that permits a process's physical address space to be non-contiguous. This technique avoids external fragmentation.

- **Core Mechanism:**
  - Physical memory is divided into fixed-sized blocks called **frames**.
  - Logical memory is divided into blocks of the same size called **pages**.
  - When a program runs, its pages are loaded into any available frames from a free-frame list.
- **Address Translation:** The CPU-generated logical address is split into two parts:
  - **Page number (p):** Used as an index into a **page table**.
  - **Page offset (d):** Combined with the base address from the page table to define the physical memory address.
  - The page table contains the base address (frame number) of each page in physical memory.
- **Fragmentation:** Paging eliminates external fragmentation but still suffers from **internal fragmentation** in the last page of a process. The trade-off is that smaller page sizes reduce internal fragmentation but increase the size of the page table.
- **Performance and the TLB:**
  - Since the page table is stored in main memory, a paged system requires two memory accesses for each data/instruction access: one for the page table and one for the actual data/instruction.
  - To solve this, a special, fast-lookup hardware cache called a **Translation Look-aside Buffer (TLB)** or associative memory is used. The TLB contains recently used page-to-frame mappings.
  - **Effective Access Time (EAT):** The performance depends on the TLB hit ratio (α). For a memory access time of 100ns and a TLB search time of 20ns, an 80% hit ratio results in an EAT of `0.80 * (100) + 0.20 * (200) = 120ns`. A 99% hit ratio improves this to `0.99 * (100) + 0.01 * (200) = 101ns`.
- **Memory Protection:** Protection is implemented by associating bits with each entry in the page table.
  - **Protection Bits:** Indicate if a page is read-only, read-write, etc.
  - **Valid-Invalid Bit:** A 'valid' bit indicates the page is in the process's logical address space. An 'invalid' bit indicates it is not, causing a trap if accessed.
- **Shared Pages:** Paging allows for the sharing of common code. One copy of read-only (reentrant) code, like a text editor, can be shared among multiple processes. Each process has its own page table, but the entries for the shared code point to the same physical frames.

#### Segmentation

Segmentation is a memory-management scheme that supports the user's view of memory. A program is seen as a collection of logical units called **segments** (e.g., main program, procedure, function, stack).

- **Logical Address:** Consists of a two-part tuple: `<segment-number, offset>`.
- **Segmentation Architecture:**
  - A **segment table** maps the logical address to a physical address. Each entry contains the **base** (starting physical address) and **limit** (length) of the segment.
  - The **Segment-Table Base Register (STBR)** points to the segment table's location in memory.
  - The **Segment-Table Length Register (STLR)** indicates the number of segments used by the program.
- **Issues:** Since segments are of variable length, memory allocation becomes a dynamic storage-allocation problem, and it is susceptible to external fragmentation.

### 4. Structuring the Page Table for Large Address Spaces

For a 32-bit logical address space with 4KB pages, a single page table can require 4 MB of contiguous memory. This is inefficient. For 64-bit systems, it's infeasible. Several techniques address this issue.

#### Hierarchical (Multi-level) Paging

This technique involves "paging the page table." The logical address space is broken up into multiple page tables.

- **Two-Level Paging:** The page number is divided into two parts: an index into an outer page table (p1) and a displacement within the page of the inner page table (p2). This allows the inner page tables to be paged in and out as needed.
- **Multi-Level Paging:** For 64-bit architectures, more levels are required. For example, a three- or four-level scheme may be used, but this increases the number of memory accesses required for translation.

#### Hashed Page Tables

Common for address spaces larger than 32 bits. The virtual page number is hashed into a page table.

- The page table contains a chain of elements that hash to the same location to handle collisions.
- Each element in the chain contains the virtual page number, the mapped page frame, and a pointer to the next element.
- **Clustered page tables** are a variation where each entry refers to several pages, which is useful for sparse address spaces.

#### Inverted Page Tables

This approach reverses the standard structure. Instead of one page table per process, there is one system-wide table with an entry for each physical frame of memory.

- Each entry contains the virtual address of the page stored in that frame and the process ID of the owner.
- **Advantage:** Decreases the total memory needed for page tables significantly.
- **Disadvantage:** Increases the time needed to search the table, as it's no longer a simple index lookup. A hash table is typically used to limit the search. Shared memory is more difficult to implement with this scheme.

### 5. Real-World Architecture Examples

| Architecture              | Key Memory Management Features                                                                                                                                                                                                                                                                                                 |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Intel IA-32 (32-bit)**  | - Supports both **segmentation and paging**. Logical address -> `segmentation unit` -> linear address -> `paging unit` -> physical address. <br> - Uses a two-level paging scheme for 4KB or 4MB pages. <br> - **Page Address Extension (PAE)** uses a three-level scheme to allow 32-bit apps to access up to 64GB of memory. |
| **Intel x86-64 (64-bit)** | - Implements 48-bit addressing in practice. <br> - Uses a **four-level paging hierarchy**. <br> - Supports page sizes of 4 KB, 2 MB, and 1 GB.                                                                                                                                                                                 |
| **ARM (32-bit)**          | - Dominant in mobile devices. <br> - Uses one-level paging for large "sections" (1MB or 16MB) and **two-level paging** for smaller pages (4KB or 16KB). <br> - Features a **two-level TLB** structure (micro TLBs and a main TLB) for fast lookups.                                                                            |

---

### Exam Preparation Enhancements

- **Mnemonic Device for Allocation Algorithms:**
  - **First-Fit:** First one that works (fastest search).
  - **Best-Fit:** Smallest hole that works (best for saving space, but creates tiny leftover holes).
  - **Worst-Fit:** Largest hole that works (worst for utilization, leaves large but potentially more useful leftover holes).
- **Key Phrase for Paging vs. Segmentation:**
  - _Paging is invisible to the programmer; Segmentation is visible._ Paging is an OS/hardware mechanism, while segments correspond to logical program units.
- **Described Diagram: TLB Operation:**
  - Imagine a flowchart. The CPU generates a logical address containing a page number `p` and offset `d`. The MMU first checks the TLB for `p`.
    - **If TLB Hit:** The frame number `f` is retrieved from the TLB. The physical address `f+d` is formed and sent to memory. This is very fast.
    - **If TLB Miss:** The MMU must access the full page table in main memory to find the entry for `p` and get the frame `f`. This is slow. The `p -> f` mapping is then added to the TLB before the physical address is formed. This miss incurs the penalty of an extra memory access.

### Quick Review Checklist

- **Logical vs. Physical Address:** CPU generates logical; memory hardware uses physical. MMU translates.
- **Address Binding Stages:** Compile time (absolute), Load time (relocatable), Execution time (dynamic).
- **Fragmentation:**
  - **External:** Unused memory is not contiguous (Contiguous Allocation, Segmentation). Solved by **compaction**.
  - **Internal:** Allocated partition is larger than needed (Paging).
- **Core Schemes & Trade-offs:**
  - **Contiguous Allocation:** Simple, fast, but suffers from external fragmentation.
  - **Paging:** Solves external fragmentation, allows non-contiguous allocation, but has internal fragmentation and page table overhead.
  - **Segmentation:** Matches user's view, but has external fragmentation.
- **Hardware Essentials:**
  - **MMU:** Translates logical to physical addresses at runtime.
  - **TLB:** A hardware cache for page table entries to speed up translation.
- **Solutions for Large Page Tables:**
  1. **Hierarchical Paging:** Page the page table.
  2. **Hashed Page Tables:** Hash virtual page number.
  3. **Inverted Page Tables:** One entry per physical frame.
- **Swapping:** Moves processes to a backing store. High overhead, not used on mobile due to flash memory limits.
- **EAT (Effective Access Time):** Performance of a paged system depends heavily on the TLB hit ratio. `EAT = (hit_ratio * time_on_hit) + (miss_ratio * time_on_miss)`.
